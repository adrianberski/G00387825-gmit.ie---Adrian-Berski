{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The analysis of a real-world phenomenon for Programming for Data Analysis.<h1>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Introductory remarks.\n",
    "\n",
    "\n",
    "A data set shall be interpreted as a collection of statistical data which is usually included in a tabulated form. It is important to mention that most often, the columns correspond to the observed statistical characteristics and each row describes one observation from the sample. The matrix cell values describe the implementation of variable data in subsequent observations.\n",
    "\n",
    "\n",
    "The purpose of this project is to research the data set and write documentation and code (in Python programming language) to investigate it. It`s important to mention that Data scientist can be interpreted as a process of experimentation and exploration to find answers. Therefore, data science pursues the various scientific method. The data science process includes steps particular to working with large, digital datasets. Before the real-world phenomenon was choose, every scientist needs to follow crucial steps:\n",
    "\n",
    "- Determine the necessary data;\n",
    "- Get the data;\n",
    "- Clean and organize the data;\n",
    "- Explore the data;\n",
    "- Model the data;\n",
    "- Communicate findings.\n",
    "\n",
    "Every of each modeling and synthesise have to include the above steps of analysis. In this project three aspects are taken into account: data scope, variable relationships and data context. \n",
    "\n",
    "\n",
    "\n",
    "## Determine the necessary data.\n",
    "In thisstate of affairs, it has been used the United States Cities Database (uscities.csv). This is up-to-date (November 18th 2020) database of United States cities and towns. \n",
    "The second set is the user_data.csv which includes information about my users, such as: education, age and location. As an example of a hypothesis which proofs that my data are neccesary to collect is a question: is there a correlation between a userâ€™s location (rural or urban) and their age? Based on provided data, it can be defined what is rural and urban location as data sets don`t detrmine that information. \n",
    "Determination process helps to apporve or dispartove particular hypotesis and helps to define how much data are needed to collect. To do this, Sample size calculators (available online) are the best tools. They shows: Margin of error, Confidence level, Population size Likely sample proportion.\n",
    "As data sets for this project have more than 200 entries, they met criertia.\n",
    "\n",
    "\n",
    "\n",
    "## Get the data.\n",
    "This project requires passive data collection, which means they are already exist. It uses data from the Simple Maps website contains a United States Cities database (the proper link has been included in the List of sourxes.). As it was mentioned above, the project requires to cross-reference of 2 provided data sets. The 2 files: uscities.csv and user_data.csv are attached to repository. \n",
    "\n",
    "\n",
    "## Clean and organize the data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               city        education  age\n0      Brooklyn, NY          college   31\n1      Brooklyn, NY  graduate degree   31\n2      Brooklyn, NY  graduate degree   32\n3      Brooklyn, NY          college   37\n4      Brooklyn, NY          college   21\n..              ...              ...  ...\n95    Baltimore, MD          college   32\n96    Baltimore, MD          college   33\n97  Los Angeles, CA          college   34\n98  Los Angeles, CA          college   35\n99  Los Angeles, CA          college   36\n\n[100 rows x 3 columns]\n              city      city_ascii state_id  state_name  county_fips  \\\n0    Prairie Ridge   Prairie Ridge       WA  Washington        53053   \n1           Edison          Edison       WA  Washington        53057   \n2         Packwood        Packwood       WA  Washington        53041   \n3   Wautauga Beach  Wautauga Beach       WA  Washington        53035   \n4           Harper          Harper       WA  Washington        53035   \n..             ...             ...      ...         ...          ...   \n95         Spangle         Spangle       WA  Washington        53063   \n96       Blanchard       Blanchard       WA  Washington        53057   \n97       Artondale       Artondale       WA  Washington        53053   \n98       May Creek       May Creek       WA  Washington        53061   \n99            Home            Home       WA  Washington        53053   \n\n   county_name      lat       lng  population  population_proper  density  \\\n0       Pierce  47.1443 -122.1408         NaN                NaN   1349.8   \n1       Skagit  48.5602 -122.4311         NaN                NaN    127.4   \n2        Lewis  46.6085 -121.6702         NaN                NaN    213.9   \n3       Kitsap  47.5862 -122.5482         NaN                NaN    261.7   \n4       Kitsap  47.5207 -122.5196         NaN                NaN    342.1   \n..         ...      ...       ...         ...                ...      ...   \n95     Spokane  47.4297 -117.3813       297.0              297.0    322.0   \n96      Skagit  48.5934 -122.4166         NaN                NaN     39.7   \n97      Pierce  47.3023 -122.6398         NaN                NaN    350.1   \n98   Snohomish  47.8555 -121.6742         NaN                NaN    301.6   \n99      Pierce  47.2790 -122.7744         NaN                NaN    198.6   \n\n     source  incorporated             timezone         zips          id  \n0   polygon         False  America/Los_Angeles  98360 98391  1840037882  \n1   polygon         False  America/Los_Angeles        98232  1840017314  \n2   polygon         False  America/Los_Angeles        98361  1840025265  \n3     point         False  America/Los_Angeles        98366  1840037725  \n4     point         False  America/Los_Angeles        98366  1840037659  \n..      ...           ...                  ...          ...         ...  \n95  polygon          True  America/Los_Angeles        99031  1840022290  \n96    point         False  America/Los_Angeles        98232  1840037747  \n97  polygon         False  America/Los_Angeles        98335  1840037617  \n98  polygon         False  America/Los_Angeles        98251  1840037857  \n99  polygon         False  America/Los_Angeles        98349  1840023831  \n\n[100 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "#importing relevant libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the 2 CSV files and create the DataFrames:\n",
    "user_data = pd.read_csv(\"user_data.csv\")\n",
    "uscities = pd.read_csv(\"uscities.csv\")\n",
    "\n",
    "# becasue of Pana library the set can be transform to a table(showing 100 records):\n",
    "\n",
    "print(user_data.head(100))\n",
    "print(uscities.head(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I need to merge both files in order to analyse the abobe hypothesis. I will use Panda`s modulde: merging \n",
    "new_df = pd.merge(user_data, uscities)\n",
    "\n",
    "#now I am testing my code:\n",
    "#print(new_df.head(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          city        education  age   city_ascii state_id  state_name  \\\n0  Kansas City  graduate degree   21  Kansas City       MO    Missouri   \n1  Kansas City  graduate degree   21  Kansas City       KS      Kansas   \n2  Albuquerque  graduate degree   28  Albuquerque       NM  New Mexico   \n\n   county_fips county_name      lat       lng  population  population_proper  \\\n0        29095     Jackson  39.1239  -94.5541   1590762.0           481420.0   \n1        20209   Wyandotte  39.1234  -94.7443    151709.0           151709.0   \n2        35001  Bernalillo  35.1055 -106.6476    759517.0           559277.0   \n\n   density   source  incorporated         timezone  \\\n0    590.0  polygon          True  America/Chicago   \n1    469.0  polygon          True  America/Chicago   \n2   1147.0  polygon          True   America/Denver   \n\n                                                zips          id location  \n0  64163 64164 64165 64167 64161 64053 64119 6411...  1840008535    urban  \n1  66101 66102 66103 66104 66105 66106 66109 6611...  1840001626    urban  \n2  87121 87123 87112 87113 87110 87111 87116 8711...  1840019176    urban  \n"
     ]
    }
   ],
   "source": [
    "#new_def shall be intepreted as thr new DataFrame, where I can see population. However to analyse the hypothesis I need location, which I can define based on the population:\n",
    "new_df.loc[new_df.population_proper < 100000, \"location\"] = \"rural\"\n",
    "new_df.loc[new_df.population_proper >= 100000, \"location\"] = \"urban\"\n",
    "#testing my findings:\n",
    "print(new_df.head(15))\n"
   ]
  },
  {
   "source": [
    "## Explore the data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## List of sources.\n",
    "\n",
    "***\n",
    "\n",
    "<b> 1) DOCUMENTS AND REPORTS.</b>\n",
    "\n",
    "Datasets: \n",
    "<http://www.cs.toronto.edu/~delve/data/>\n",
    "\n",
    "UCI Machine Learning Repository:\n",
    "<http://archive.ics.uci.edu/ml/datasets/Iris>\n",
    "\n",
    "NumPy Documentation:\n",
    "<https://numpy.org/doc/stable/reference/random/>\n",
    "\n",
    "NumPy v1.19 Manual:\n",
    "<https://numpy.org/doc/stable/index.html>\n",
    "\n",
    "NumPy in general:\n",
    "<https://numpy.org/>\n",
    "\n",
    "\n",
    "<b> 2) ARTICLES, MONOGRAPHS, STUDIES.</b>\n",
    "\n",
    "Encyclopaedia Britannica: \n",
    "<http://www.britannica.com/>\n",
    "\n",
    "Schweppes J., How to Think Write and Cite â€“ key skills for Irish Law, Dublin 2011.\n",
    "\n",
    "Waugh S., Extending and benchmarking Cascade-Correlation, Tasmania 1995.\n",
    "\n",
    "Internet Resources: \n",
    "\n",
    "<https://www.w3schools.com/python/numpy_random.asp>\n",
    "\n",
    "<https://www.geeksforgeeks.org/numpy-random-rand-python/>\n",
    "\n",
    "<https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.random.html>\n",
    "\n",
    "<https://www.markdownguide.org/getting-started/>\n",
    "\n",
    "<https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed>\n",
    "\n",
    "<https://github.com/>\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Triangular_distribution>\n",
    "\n",
    "<https://gitter.im/GMIT-Python-Learners-2019/community>\n",
    "\n",
    "<https://docs.python.org/3/tutorial/>\n",
    "\n",
    "<https://leanpub.com/pyprog/read>\n",
    "\n",
    "<https://www.codecademy.com/courses/data-visualization-python/lessons/matplotlib-i/exercises/introduction-matplotlib-i>\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}